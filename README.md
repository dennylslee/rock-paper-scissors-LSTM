# Introduction

This project uses the popular Long Short Term Memory (LSTM) Recurrent Neural Network as an AI agent to play the simple rock-paper-scissors (RPS) game against another mock player. 

Player 1: RPS sequence is generated by various pseudo-random number generator. Unlike a real RPS game,  player 1's moves are independent of player 2 (AI agent).  Several intentionally weak(er) PRNG are used to assess the performance of the AI agent. 

Player 2 (AI agent): RPS sequence is implemented using LSTM.  The LSTM performs a time series prediction of the player 1's next move based on the observed previous moves' sequence. 

Programs contained in this project:
1. a set of files with "-datagen" in the filename are generating the RPS sequence.  The output text file is simply a long string of "ppsrpsrrsp..."
2. rps-RNN-LSTM.py for constructing the LSTM 

The LSTM is built on Keras framework. For some of the best LSTM tutorials I have encontered, go to [Colah's Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) and [Shi Yan blog on understanding LSTM](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714)

## RPS sequence generation

There are six variants of input sequences available for testing:

```python
# ---------------------------------------load player 1 play sequence-----------------------------------

#raw_rps_seq = load_doc("player1_rps_PRNG.txt")					# long psuedo random generation w distribution
#raw_rps_seq = load_doc("player1_rps_RandRepeat.txt")			# random and deterministic repeated sequence 
#raw_rps_seq = load_doc("player1_rps_LFSR.txt")					# linear feedback shift register as PRNG
raw_rps_seq = load_doc("player1_rps_RANDU.txt")					# RANDU algorithm as PRNG
#raw_rps_seq = load_doc("player1_rps_p1_RawRoshambo.txt")		# player 1 dataset from Roshambo human dataset
#raw_rps_seq = load_doc("player1_rps_p2_RawRoshambo.txt")		# player 2 dataset from Roshambo human dataset
```


### PRNG sequence generated using the python built-in guassian distribution

The mode (bell) of the guassian distribution control which type of move is more frequent.  Use the sigma value to control how "flat" the desired bell curve is. The larger the sigma, the larger the bell and hence it starts to resemble the uniform distribution which makes prediction more/very challegnaging. Note that the cutoff threshold is hard coded to 1 and -1 with respect to the norm of zero.

The condition if statement is put in to allow for option to alter the move strategy mid way through the generation sequence. You can see that AI agent once trained on the prior strategy would start to loss if player 1 strategy is altered.  

```python
player1_list_size = 5000	# the length of the series
norm_mu = 0 				# center the guassian mean at zero
norm_signma = 2				# sigma determine the spread
player1_list = []
player1_list_quant = []

# write the list into file

random.seed(11)

for i in range(player1_list_size):
	a = random.gauss(norm_mu,norm_signma) # use built in gaussian distribution PRNG
	if i <= player1_list_size / 2:	# disturb the random seed half way through intentionally
		if a < -1:				# cutoff point for value below -1 
			a_quant = "r"			# value 1 = ROCK
		elif a > 1:				# cutoff point for value above +1
			a_quant = "s"			# value 3 = SESSIORS
		else:					# everything else in between
			a_quant = "p"			# value 2 =  PAPER
	else:						# neutralize the if statement when not in use
		if a < -1:				# cutoff point for value below -1 
			a_quant = "r"			# value 1 = ROCK
		elif a > 1:				# cutoff point for value above +1
			a_quant = "s"			# value 3 = SESSIORS
		else:					# everything else in between
			a_quant = "p"			# value 2 =  PAPER
	player1_list.append(a)
	player1_list_quant.append(a_quant)
print (player1_list_quant[:50])
print (player1_list_quant.count("r"))	# count the num of ones in list (rock)
print (player1_list_quant.count("p"))	# count the num of twos in list (paper)
print (player1_list_quant.count("s"))	# count the num of threes in list (sessiors)
with open("player1_rps_PRNG.txt","w") as f :	
	for row in player1_list_quant:
		f.write(row)
```


### Random Repeat as sequence generator

This is simply an arbitrarily generated sequence of rps of any length repeated N times. Since the sequence is generally shorter than any PRNG (even the weak ones), this sequence is the most winable.

```python
repeat = 50
randomstr = 'sprrpsrpssprrpsrppsprrspsspprrssssppspspsrspssprpsrsprrspprsppspsprrssprsprspsppss'
with open("player1_rps_RandRepeat.txt","w") as f :	
	for row in range(repeat):
		f.write(randomstr)

print (repeat * randomstr.count("r"))	# count the num of ones in list (rock)
print (repeat * randomstr.count("p"))	# count the num of twos in list (paper)
print (repeat * randomstr.count("s"))	# count the num of threes in list (sessiors)
```

### Linear Feedback Shift Register as sequence generator

Since a sophisticated PRNG proves to hard to predict, a weak form of PRNG is used to see if it makes any difference.  LFSR is easy to understand and implement. The reference to the basics can be found [here](https://en.wikipedia.org/wiki/Linear-feedback_shift_register). The 12bits version is used in this project.

### RANDU as sequence generator

Another notoriously weak PRNG is the RANDU algorithm.  The reference to the basics can be found [here](https://en.wikipedia.org/wiki/RANDU)

### Human-generated sequences

There are various online and app games for human to interact with an algorithmically driven RPS player. This includes the NYTimes and Roshambo. Two human sequences from the Roshambo app were used in this project.  Note that in this project, the sequence is simply play out independently without any consideration of player 2's moves. But in fact the origianl sequences was generated with countermove psychology in play. 


# RPS move prediction using LSTM

The main purpose of the project is to assess the win-ability performance of using LSTM as player 2 against a randomly generated player 1 sequence. For testing purpose player 2 can also be assigned as a random player.  The switch "play_AI" controls if player2 is LSTM or random. 

Other main control parameters of the LSTM are below. Note that:
1. online mode switches the performance to the training phase and treat it like a continous self-learning sequence (i.e. a form of perpetual model fitting
2 stackLSTM enables an extra layers in the LSTM.  It turns out that this has little effects on the results given the sequence pattern has no hierarchical structure. 


```python
#------------------------------------------config section ------------------------------------------------------------------
play_AI = True				# player 2 is LSTM
onlineMode = True 			# emulate online self-learning mode - use training phase as test reference; for batch size to 1
stackLSTM = True			# stack up multple layers of LSTM
output_dim = 3				# fix this at 3 for RPS use case
input_dim = 3				# fix this at 3 for RPS use case
timestep_length = 100		# play with this for number of hidden nodes in LSTM
training_pct = 0.7			# percentage of overall dataset used for training
dropout = 0.7				# dropout rate (1 means full drop out)
epochs = 25					# num of epochs (one epoch is one sweep of full training set)
hiddenUnits = 10			# size of the hidden units in each cell
validation_split = 0.2		# percentage split in model.fit for built in validation during training phase
np.random.seed(7)			# fix a random seed
# num of training samples submitted for one fwd/bwd pass before weights are updated
batch_size = 1 if onlineMode else 4   
```

The main LSTM model construction:

```python
model = Sequential()
model.add(LSTM(hiddenUnits, return_sequences=stackLSTM, input_shape=(timestep_length,input_dim))) 
if stackLSTM:
	model.add(Dropout(dropout))
	model.add(LSTM(hiddenUnits))
model.add(Dropout(dropout))								# stack LSTM architecture;  does not do much in this case	 	
model.add(Dense(output_dim, activation='softmax'))			# the number of unit needs to match the output dim size
print(model.summary())
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# fit the model ; tensorboard will require validation data (or created by split)
tbCallBack = keras.callbacks.TensorBoard(log_dir='./logs/'+now, histogram_freq=1,  write_graph=True, write_grads=True, write_images=False)
model.fit(
	trainX, 
	trainY, 
	epochs=epochs, 
	batch_size=batch_size, 
	verbose=2,
	validation_split = validation_split, 
	callbacks = [tbCallBack])
```

# Main tuning notes

1. Need sufficient training samples
2. timesteps sequence size is immportant.  For repeating sequence, a longer timesteps size provide a longer memory window but for random sequence, a long timesteps tends to cause overfitting. 
3. dropout is useful in countering the tendency to overfit
4. number of epochs does not have much effects in this project since it seems to reach the optimal loss point pretty fast. 
5. hidden units is the vector size of the cell and hidden state of the LSTM architecture. The larger it is the more information carrying ability of the LSTM but it can also cause overfitting. 

# Performance assessment 
