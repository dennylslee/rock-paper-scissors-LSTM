# Introduction

This project uses the popular Long Short Term Memory (LSTM) Recurrent Neural Network as an AI agent to play the simple rock-paper-scissors (RPS) game against another mock player. 

Player 1: RPS sequence is generated by various pseudo-random number generator. Unlike a real RPS game,  player 1's moves are independent of player 2 (AI agent).  Several intentionally weak(er) PRNG are used to assess the performance of the AI agent. 

Player 2 (AI agent): RPS sequence is implemented using LSTM.  The LSTM performs a time series prediction of the player 1's next move based on the observed previous moves' sequence. 

Programs contained in this project:
1. a set of files with "-datagen" in the filename are generating the RPS sequence.  The output text file is simply a long string of "ppsrpsrrsp..."
2. rps-RNN-LSTM.py for constructing the LSTM 

The LSTM is built on Keras framework. For some of the best LSTM tutorials I have encontered, go to [Colah's Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) and [Shi Yan blog on understanding LSTM](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714)

## RPS sequence generation

There are six variants of input sequences available for choosing:

```python
# ---------------------------------------load player 1 play sequence-----------------------------------

#raw_rps_seq = load_doc("player1_rps_PRNG.txt")					# long psuedo random generation w distribution
#raw_rps_seq = load_doc("player1_rps_RandRepeat.txt")			# random and deterministic repeated sequence 
#raw_rps_seq = load_doc("player1_rps_LFSR.txt")					# linear feedback shift register as PRNG
raw_rps_seq = load_doc("player1_rps_RANDU.txt")					# RANDU algorithm as PRNG
#raw_rps_seq = load_doc("player1_rps_p1_RawRoshambo.txt")		# player 1 dataset from Roshambo human dataset
#raw_rps_seq = load_doc("player1_rps_p2_RawRoshambo.txt")		# player 2 dataset from Roshambo human dataset
```


### PRNG sequence generated using the python built-in guassian distribution

The mode (bell) of the guassian distribution control which type of move is more frequent.  Use the sigma value to control how "flat" the desired bell curve is. The larger the sigma, the larger the bell and hence it starts to resemble the uniform distribution which makes prediction more/very challegnaging. Note that the cutoff threshold is hard coded to 1 and -1 with respect to the norm of zero.

The condition if statement is put in to allow for option to alter the move strategy mid way through the generation sequence. You can see that AI agent once trained on the prior strategy would start to loss if player 1 strategy is altered.  

```python
player1_list_size = 5000	# the length of the series
norm_mu = 0 				# center the guassian mean at zero
norm_signma = 2				# sigma determine the spread
player1_list = []
player1_list_quant = []

# write the list into file

random.seed(11)

for i in range(player1_list_size):
	a = random.gauss(norm_mu,norm_signma) # use built in gaussian distribution PRNG
	if i <= player1_list_size / 2:	# disturb the random seed half way through intentionally
		if a < -1:				# cutoff point for value below -1 
			a_quant = "r"			# value 1 = ROCK
		elif a > 1:				# cutoff point for value above +1
			a_quant = "s"			# value 3 = SESSIORS
		else:					# everything else in between
			a_quant = "p"			# value 2 =  PAPER
	else:						# neutralize the if statement when not in use
		if a < -1:				# cutoff point for value below -1 
			a_quant = "r"			# value 1 = ROCK
		elif a > 1:				# cutoff point for value above +1
			a_quant = "s"			# value 3 = SESSIORS
		else:					# everything else in between
			a_quant = "p"			# value 2 =  PAPER
	player1_list.append(a)
	player1_list_quant.append(a_quant)
print (player1_list_quant[:50])
print (player1_list_quant.count("r"))	# count the num of ones in list (rock)
print (player1_list_quant.count("p"))	# count the num of twos in list (paper)
print (player1_list_quant.count("s"))	# count the num of threes in list (sessiors)
with open("player1_rps_PRNG.txt","w") as f :	
	for row in player1_list_quant:
		f.write(row)
```


### Random Repeat as sequence generator

This is simply an arbitrarily generated sequence of rps of any length repeated N times. Since the sequence is generally shorter than any PRNG (even the weak ones), this sequence is the most winable.

```python
repeat = 50
randomstr = 'sprrpsrpssprrpsrppsprrspsspprrssssppspspsrspssprpsrsprrspprsppspsprrssprsprspsppss'
with open("player1_rps_RandRepeat.txt","w") as f :	
	for row in range(repeat):
		f.write(randomstr)

print (repeat * randomstr.count("r"))	# count the num of ones in list (rock)
print (repeat * randomstr.count("p"))	# count the num of twos in list (paper)
print (repeat * randomstr.count("s"))	# count the num of threes in list (sessiors)
```

### Linear Feedback Shift Register as sequence generator

Since a sophisticated PRNG proves to hard to predict, a weak form of PRNG is used to see if it makes any difference.  LFSR is easy to understand and implement. The reference to the basics can be found [here](https://en.wikipedia.org/wiki/Linear-feedback_shift_register). The 12bits version is used in this project.

### RANDU as sequence generator

Another notoriously weak PRNG is the RANDU algorithm.  The reference to the basics can be found [here](https://en.wikipedia.org/wiki/RANDU).

### Human-generated sequences

There are various online and app games for human to interact with an algorithmically driven RPS player. This includes the NYTimes and Roshambo. Two human sequences from the Roshambo app were used in this project.  Note that in this project, the sequence is simply play out independently without any consideration of player 2's moves. But in fact the origianl sequences was generated with countermove psychology in play. 


# RPS move prediction using LSTM

The main purpose of the project is to assess the win-ability performance of using LSTM as player 2 against a randomly generated player 1 sequence. For testing purpose player 2 can also be assigned as a random player.  The switch "play_AI" controls if player2 is LSTM or random. 

Other main control parameters of the LSTM are below. Note that:
1. online mode switches the performance to the training phase and treat it like a continous self-learning sequence (i.e. a form of perpetual model fitting.
2.
 stackLSTM enables an extra layers in the LSTM.  It turns out that this has little effects on the results given the sequence pattern has no hierarchical structure. 


```python
#------------------------------------------config section ------------------------------------------------------------------
play_AI = True				# player 2 is LSTM
onlineMode = True 			# emulate online self-learning mode - use training phase as test reference; for batch size to 1
stackLSTM = True			# stack up multple layers of LSTM
output_dim = 3				# fix this at 3 for RPS use case
input_dim = 3				# fix this at 3 for RPS use case
timestep_length = 100		# play with this for number of hidden nodes in LSTM
training_pct = 0.7			# percentage of overall dataset used for training
dropout = 0.7				# dropout rate (1 means full drop out)
epochs = 25					# num of epochs (one epoch is one sweep of full training set)
hiddenUnits = 10			# size of the hidden units in each cell
validation_split = 0.2		# percentage split in model.fit for built in validation during training phase
np.random.seed(7)			# fix a random seed
# num of training samples submitted for one fwd/bwd pass before weights are updated
batch_size = 1 if onlineMode else 4   
```

The main LSTM model construction:

```python
model = Sequential()
model.add(LSTM(hiddenUnits, return_sequences=stackLSTM, input_shape=(timestep_length,input_dim))) 
if stackLSTM:
	model.add(Dropout(dropout))
	model.add(LSTM(hiddenUnits))
model.add(Dropout(dropout))								# stack LSTM architecture;  does not do much in this case	 	
model.add(Dense(output_dim, activation='softmax'))			# the number of unit needs to match the output dim size
print(model.summary())
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# fit the model ; tensorboard will require validation data (or created by split)
tbCallBack = keras.callbacks.TensorBoard(log_dir='./logs/'+now, histogram_freq=1,  write_graph=True, write_grads=True, write_images=False)
model.fit(
	trainX, 
	trainY, 
	epochs=epochs, 
	batch_size=batch_size, 
	verbose=2,
	validation_split = validation_split, 
	callbacks = [tbCallBack])
```

## LSTM tuning

1. Need sufficient training samples.
2. timesteps sequence size is immportant.  For repeating sequence, a longer timesteps size provide a longer memory window but for random sequence, a long timesteps tends to cause overfitting. 
3. dropout is useful in countering the tendency to overfit.
4. number of epochs does not have much effects in this project since it seems to reach the optimal loss point pretty fast. 
5. hidden units is the vector size of the cell and hidden state of the LSTM architecture. The larger it is the more information carrying ability of the LSTM but it can also cause overfitting. 

# Perforamnce assessment 

The measurment of the LSTM AI agent is measured by the culminated win, tie, loss rate. Note that the final output stage of the LSTM is based on softmax which is effectively a prediction probability value of each move. An "argmax" function is used to pick out the highest likelihood of the next move.  (side note: I tried using softsign as suggested by some but has not effect to my result)

* Green line: win rate
* Black line: tie rate
* Red line: loss rate

## Playing against PRNG

Using the guassian PRNG as sequence generator, the LSTM can learn to beat player 2 in the long run as shown below.  However, it was not able to detect any sequence pattern.  It seems to only learn that probablisitically, there is a higher occurance of certain move type and it simply puts out counter move all the time.  I cannot tune the LSTM makes it perform any better than the steady state of ~38-40% win rate. 

If the original sequence bell curve distribution is flatten to resemble uniform distribution. The win-tie-loss rate goes to the random equalibrium level of 33% each.

PRNG generator using guassian distribution with sigma = 2.
![result 1](https://github.com/dennylslee/rock-paper-scissors-LSTM/PRNG-result-1.png)

PRNG generator using guassian distribution with sigma = 2.3.  
![result 2](https://github.com/dennylslee/rock-paper-scissors-LSTM/PRNG-result-2.png)

## Playing against Random Repeat

LSTM clearly demonstrates its ability to learn the long sequence and generate counter-moves consistently (with steady state win rate at ~70%).  

![result 3](https://github.com/dennylslee/rock-paper-scissors-LSTM/RandRpeat-result-1.png)

The last 20 moves are captured to illustrate the move sequence of the LSTM:

```
Last 20 moves of player 1 versus player 2
0 = rock, 1 = paper, 2 = sciessors
player 1: [1, 2, 1, 2, 1, 0, 0, 2, 2, 1, 0, 2, 1, 0, 2, 1, 2, 1, 1]
player 2: [2, 1, 2, 2, 2, 1, 1, 0, 0, 2, 0, 0, 2, 1, 1, 2, 0, 2, 1]
```

The dropout value has to be lowered to achieve better result given the tuning parameters set.
```python
timestep_length = 10		# play with this for number of hidden nodes in LSTM
training_pct = 0.2			# percentage of overall dataset used for training
dropout = 0.2				# dropout rate (1 means full drop out)
epochs = 25					# num of epochs (one epoch is one sweep of full training set)
hiddenUnits = 10			# size of the hidden units in each cell
```

## Playing against 12bits LFSR

The LFSR is a weak PRNG and rps is generated based on binning the generated random number. The LSTM win rate is 50% while lossing and tie-ing 33%:

![result 4](https://github.com/dennylslee/rock-paper-scissors-LSTM/LFSR-result-1.png)

The last 20 moves are captured to illustrate the move sequence of the LSTM:

```
Last 20 moves of player 1 versus player 2
0 = rock, 1 = paper, 2 = sciessors
player 1: [2, 1, 0, 0, 2, 1, 2, 1, 2, 1, 2, 2, 0, 1, 2, 1, 2, 1, 2]
player 2: [0, 2, 0, 1, 1, 2, 0, 2, 0, 2, 0, 2, 2, 1, 0, 2, 0, 2, 0]
```
## Playing against RANDU

Though RANDU is itself a weak PRNG, the random number generated looks to be uniformly distributed and it proves to be tricky for the LSTM to learn. Some careful tuning of the LSTM parameters allow some consistent but only marginally better win rate in steady state. 

![result 5](https://github.com/dennylslee/rock-paper-scissors-LSTM/RANDU-result-2.png)

The main parameters:

```python
timestep_length = 100		# play with this for number of hidden nodes in LSTM
training_pct = 0.3			# percentage of overall dataset used for training
dropout = 0.3				# dropout rate (1 means full drop out)
epochs = 25					# num of epochs (one epoch is one sweep of full training set)
hiddenUnits = 50			# size of the hidden units in each cell

```

Interestingly, increaing the number of timesteps (to 200) or lowering it results in worse performance of win rate at 33%. The theory is that too many timesteps causes overfitting, which we would need to compensate with more dropout.  Too little timesteps constrains the memory time window for the LSTM to learn the sequence pattern.  More optimization can be achieved here. 

# Other things to try in the future:

* turning on the online mode and learn from a longer sequence of RANDU to see if we can improve the result (since using longer timestep seems to have better result)
* relationship between timestep_length, dropout, and hiddenstate can be more finely tuned to potentially get better results while minimizing overfitting
